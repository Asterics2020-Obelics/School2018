{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics: Hands On 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build models for a triggered photon counter with a fixed exposure time. Assume that the number of photons $n$ entering the detector follows a [Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution):\n",
    "$$\n",
    "P(n\\mid \\lambda) = e^{-\\lambda}\\, \\frac{\\lambda^n}{n!}\n",
    "$$\n",
    "You will first create a **probabilistic model** for this photon counter, i.e., code to evaluate the likelihood $P(n\\mid \\lambda)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Fill in the function below to calculate $\\log P(n\\mid \\lambda)$ without using any statistical libraries.\n",
    "\n",
    "Notes:\n",
    " - `lambda` is a reserved keyword in python, so we use `lam` instead.\n",
    " - Use [scipy.special.gammaln](https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.gammaln.html) to evaluate $\\log n! = \\log\\Gamma(n + 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(n, lam):\n",
    "    assert n >= 0\n",
    "    # Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your probabilities are correctly normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in 0.5, 1., 2.:\n",
    "    probs = [np.exp(log_likelihood(n, lam)) for n in range(100)]\n",
    "    print(f'lam={lam}, sum of probs={np.sum(probs):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reimplement your function to use [scipy.stats.poisson](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.poisson.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(n, lam):\n",
    "    assert n >= 0\n",
    "    # Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, check that your probabilities are correctly normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in 0.5, 1., 2.:\n",
    "    probs = [np.exp(log_likelihood(n, lam)) for n in range(100)]\n",
    "    print(f'lam={lam}, sum of probs={np.sum(probs):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, modify your model to account for your counter's trigger, which only outputs a value of $n$ when at least one photon is detected $n \\ge 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(n, lam):\n",
    "    assert n >= 1\n",
    "    # Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your probabilities are correctly normalized for $n \\ge 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lam in 0.5, 1., 2.:\n",
    "    probs = [np.exp(log_likelihood(n, lam)) for n in range(1, 100)]\n",
    "    print(f'lam={lam}, sum of probs={np.sum(probs):.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Next, build a corresponding **generative model** by completing the function below.\n",
    "\n",
    "Notes:\n",
    " - Use `scipy.stats.poisson` again.\n",
    " - You should never return $n=0$ since you are simulating a **triggered** counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(lam, random_state):\n",
    "    \"\"\"Generative model of triggered photon counter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lam : float\n",
    "        Mean number of photons received during an exposure (without a trigger).\n",
    "    random_state : numpy random_state object\n",
    "        Random state to use for reproducible random numbers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Number of generated photons.\n",
    "    \"\"\"\n",
    "    # Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some random samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam=2\n",
    "random_state = np.random.RandomState(seed=123)\n",
    "samples = np.array([generate(lam, random_state) for sample in range(10000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your trigger is working:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(samples > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that your samples have the expected mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(np.mean(samples), lam / (1 - np.exp(-lam)), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read some of the overlapping sources data we generated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_4 = pd.read_hdf('data/sources_4_xy.hf5')\n",
    "sources_5 = pd.read_hdf('data/sources_5_xy.hf5')\n",
    "sources_8 = pd.read_hdf('data/sources_8_xy.hf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, sharex=True, sharey=True, figsize=(8, 3))\n",
    "for col, data in enumerate((sources_4, sources_5, sources_8)):\n",
    "    ax[col].scatter(data.iloc[:, 0], data.iloc[:, 1], s=5, lw=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we can fit a mixture of Gaussians to each of these datasets, e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = mixture.GaussianMixture(n_components=2).fit(sources_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the fit results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import display_gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_gmm(sources_5, fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all fine if you know in advance how many sources the data contains, but you usually don't for interesting problems.\n",
    "\n",
    "A popular method for selecting the \"best\" number of Gaussian is to use the [Akaike information criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion) and [Bayesian information criterion (BIC)](https://en.wikipedia.org/wiki/Bayesian_information_criterion). These are different measures of the \"goodness of fit\" of the GMM to the data.  They both reward small residuals while penalizing extra fit parameters. However, both methods are ad-hoc and you should prefer (much more expensive) [Bayesian model selection methods](https://github.com/dkirkby/MachineLearningStatistics/blob/master/notebooks/ModelSelection.ipynb) when the answer really matters, or you need uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Use the methods `fit.aic()` and `fit.bic()` to plot the AIC and BIC for `n_components=1,2,3,4` with each of these three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_criteria(data):\n",
    "    # Add your code here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_criteria(sources_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_criteria(sources_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_criteria(sources_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Expectation-Maximization (EM) algorithm](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) is used to implement many machine learning methods, including several we have already studied: K-means, factor analysis and weighted PCA.\n",
    "\n",
    "The basic idea of EM is to optimize a goal function that depends on two disjoint sets of parameters by alternately updating one set and then the other, using a scheme that is guaranteed to improve the goal function (although generally to a local rather than global optimum). The alternating updates are called the E-step and M-step.\n",
    "\n",
    "The K-means is one of the simplest uses of EM, so is a good way to get some hands-on experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Implement the function below to perform a K-means E-step. Hint: you might find [np.argmin](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.argmin.html) useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step(X, centers):\n",
    "    \"\"\"Perform a K-means E-step.\n",
    "    \n",
    "    Assign each sample to the cluster with the nearest center, using the\n",
    "    Euclidean norm to measure distance between a sample and a cluster center.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    centers : array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    n = len(centers)\n",
    "    assert centers.shape[1] == D\n",
    "    indices = np.empty(N, int)\n",
    "    # Add your code here...\n",
    "\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = gen.normal(size=(20, 2))\n",
    "centers = np.array([[0., 0.], [0., 10.]])\n",
    "X[50:] += centers[1]\n",
    "indices = E_step(X, centers)\n",
    "assert np.all(indices[:50] == 0)\n",
    "assert np.all(indices[50:] == 1)\n",
    "\n",
    "centers = gen.uniform(size=(5, 2))\n",
    "indices = E_step(X, centers)\n",
    "assert np.array_equal(indices, [4, 1, 4, 4, 1, 0, 1, 0, 2, 1, 2, 4, 0, 1, 0, 1, 0, 1, 4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implement the function below to perform a K-means M-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step(X, indices, n):\n",
    "    \"\"\"Perform a K-means M-step.\n",
    "    \n",
    "    Calculate the center of each cluster as the geometric mean of its assigned samples.\n",
    "    \n",
    "    The centers of any clusters without any assigned samples should be set to the origin.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array with shape (N, D)\n",
    "        Input data consisting of N samples in D dimensions.\n",
    "    indices : integer array with shape (N,)\n",
    "        Cluster index of each sample, in the range 0 to n-1.\n",
    "    n : int\n",
    "        Number of clusters.  Must be <= N.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array with shape (n, D)\n",
    "        Centers of the the n clusters in D dimensions.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert indices.shape == (N,)\n",
    "    assert n <= N\n",
    "    centers = np.zeros((n, D))\n",
    "    # Add your code here...\n",
    "    \n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X = np.ones((20, 2))\n",
    "indices = np.zeros(20, int)\n",
    "centers = M_step(X, indices, 1)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "n = 5\n",
    "indices = gen.randint(n, size=len(X))\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.all(centers == 1.)\n",
    "\n",
    "X = gen.uniform(size=X.shape)\n",
    "centers = M_step(X, indices, n)\n",
    "assert np.allclose(\n",
    "    np.round(centers, 3),\n",
    "    [[ 0.494,  0.381], [ 0.592,  0.645],\n",
    "     [ 0.571,  0.371], [ 0.234,  0.634],\n",
    "     [ 0.250,  0.386]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now implemented the core of the K-means algorithm.  Experiment with it using this simple wrapper, which makes a scatter plot of the first two columns after each iteration. The actual sklearn wrapper combines the result of several random starting points and has other refinements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KMeans_fit(data, n_clusters, nsteps, seed=123):\n",
    "    X = data.values\n",
    "    N, D = X.shape\n",
    "    assert n_clusters <= N\n",
    "    gen = np.random.RandomState(seed=seed)\n",
    "    # Pick random samples as the initial centers.\n",
    "    shuffle = gen.permutation(N)\n",
    "    centers = X[shuffle[:n_clusters]]\n",
    "    # Perform an initial E step to assign samples to clusters.\n",
    "    indices = E_step(X, centers)\n",
    "    # Loop over iterations.\n",
    "    for i in range(nsteps):\n",
    "        centers = M_step(X, indices, n_clusters)\n",
    "        indices = E_step(X, centers)\n",
    "    # Plot the result.\n",
    "    cmap = np.array(sns.color_palette())\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=cmap[indices % len(cmap)])\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], marker='+', c='k', s=400, lw=5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this out on some randomly generated 2D data with 3 separate clusters (using the handy [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "gen = np.random.RandomState(seed=123)\n",
    "X, _ = make_blobs(500, 2, [[-3,-3],[0,3],[3,-3]], random_state=gen)\n",
    "data = pd.DataFrame(X, columns=('x0', 'x1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple test, you should find a good solution after two iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMeans_fit(data, n_clusters=3, nsteps=0);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=1);\n",
    "KMeans_fit(data, n_clusters=3, nsteps=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering in Many Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We previously looked for clusters where each sample $(x,y)$ was a point in 2D.\n",
    "\n",
    "What if we treat a whole image as one sample in 256 dimensions (one per pixel)?  Can we find clusters of images in this large dimension?\n",
    "\n",
    "Recall the parameters of the generative model:\n",
    " - Number of sources $N$ = 1-4.\n",
    " - $(x_0, y_0, \\sigma, e_1, e_2)$ for each source.\n",
    " \n",
    "What might clusters of images correlate with from this list?\n",
    "\n",
    "To simplify the problem, we will focus on the size ($\\sigma$) of images with a single isolated cluster ($N=1$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_img = pd.read_hdf('data/sources_img.hf5')\n",
    "df_par = pd.read_hdf('data/sources_par.hf5')\n",
    "isolated = df_par['num_source'] == 1\n",
    "df_iso = df_img[isolated]\n",
    "sigma = np.concatenate(df_par[isolated]['sigma'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following function to compare the size distributions of isolated sources assigned to different clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sizes(fit):\n",
    "    for k in range(fit.n_clusters):\n",
    "        plt.hist(sigma[fit.labels_ == k], bins=30, range=(0, 3.5),\n",
    "                 density=True, histtype='step', lw=2, label=f'Cluster #{k}')\n",
    "    plt.legend()\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = compare_sizes(cluster.KMeans(n_clusters=3, random_state=1).fit(df_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, all 3 clusters have essentially the same distribution of sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Look for 10 clusters instead of 3 (again using `random_state=1`). One cluster should be clearly different from the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = compare_sizes(cluster.KMeans(n_clusters=10, random_state=1).fit(df_iso))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `plot_image()` function displays a single image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import plot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plot_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(df_iso.iloc[4].values, label='#4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to identify and plot some members of this cluster, and check that they make sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the cluster finding with some different seed values for `random_state`. How robust is this cluster of large sizes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, it is difficult to visualize clusters (or anything) in more than 2 dimensions. Your options are to:\n",
    "- Use attributes such as color and size to display additional dimensions.\n",
    "- Use 3D visualizations.\n",
    "- Use an embedding transformation to map high dimensional data to 2D or 3D.\n",
    "\n",
    "Try the embedding approach by uploading our 256-dimensional data to http://projector.tensorflow.org/. You will first need to save the data in the \"tab-separated value\" (TSV) format. We will only save the first 1000 images to keep the file size small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iso[:1000].to_csv('data/df_iso.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the PCA visualization option, which reduces the dimensionality to 2 or 3 dimensions.\n",
    "\n",
    "You can also experiment with the [tSNE](https://distill.pub/2016/misread-tsne/) visualizations. These can be hypnotic but I have not found them to be very enlightening for scientific data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
